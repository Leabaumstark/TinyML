{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25147d4",
   "metadata": {},
   "source": [
    "# How to use ONNX2C to convert ML models into C-files\n",
    "## Intro\n",
    "### TinyML procedure\n",
    "Developing machine learning models used on embedded devices (socalled \"tinyML\") generally follows the following procedure:\n",
    "1. Create and train a model\n",
    "2. Convert for platform interoperability\n",
    "3. Optimization\n",
    "4. Deploying\n",
    "\n",
    "In our special case, the procedure is as follows:\n",
    "1. Create and train a model\n",
    "2. Convert to ONNX\n",
    "3. Optimization\n",
    "4. Convert to C code\n",
    "5. Use model in C code\n",
    "\n",
    "## 0. Prerequisites\n",
    "In order to manage all our python packages, we first set up a virtual environment like so (use Git Bash or adapt commands):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b9dc4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m venv .venv\n",
    "source .venv/Scripts/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2149d0",
   "metadata": {},
   "source": [
    "## 1. Create and train a model\n",
    "To have something we can convert, we first need a pretrained model. We therefore use TensorFlow Keras to create and train a model. First, we have to install the required python packages. Note that matplotlib is only for visualizing samples in this notebook and not generally necessary for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ee9a7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c117929",
   "metadata": {},
   "source": [
    "Now we can create a new python script and import the required packages. Note that keras is automatically installed with tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1744daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f5fa0",
   "metadata": {},
   "source": [
    "Next, we need an idea for what our model shall do. As an example, we will create and train a model, that recognizes handwritten digits between 0 and 9, using the [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database). Originally, the samples are 28 x 28 pixel, greyscale images. We use the dataset provided by Keras, to make things easier. Therefore, we import the dataset, normalize the pixel values, and split the dataset into training and test data. Finally, we take 10,000 samples of the training data, to later use them for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571b3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4dc58",
   "metadata": {},
   "source": [
    "Now that we have data, we need a model. We create one as follows. The creation of models in TensorFlow Keras is out of scope for this docu and will not be explained further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799e8f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\baumstark\\TinyML\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10),\n",
    "  tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.output_names=['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc58796",
   "metadata": {},
   "source": [
    "Using our data, we can now train the model. Details on that are also not covered in this docu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c6a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3955 - sparse_categorical_accuracy: 0.8851 - val_loss: 0.1755 - val_sparse_categorical_accuracy: 0.9493\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1795 - sparse_categorical_accuracy: 0.9477 - val_loss: 0.1273 - val_sparse_categorical_accuracy: 0.9655\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1404 - sparse_categorical_accuracy: 0.9588 - val_loss: 0.1072 - val_sparse_categorical_accuracy: 0.9696\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1158 - sparse_categorical_accuracy: 0.9651 - val_loss: 0.1003 - val_sparse_categorical_accuracy: 0.9710\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.0962 - val_sparse_categorical_accuracy: 0.9731\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0915 - sparse_categorical_accuracy: 0.9732 - val_loss: 0.0921 - val_sparse_categorical_accuracy: 0.9743\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9751 - val_loss: 0.0871 - val_sparse_categorical_accuracy: 0.9742\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9767 - val_loss: 0.0850 - val_sparse_categorical_accuracy: 0.9768\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0703 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0816 - val_sparse_categorical_accuracy: 0.9764\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0639 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0810 - val_sparse_categorical_accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3955153226852417,\n",
       "  0.179549440741539,\n",
       "  0.140412375330925,\n",
       "  0.11578560620546341,\n",
       "  0.10251901298761368,\n",
       "  0.09154689311981201,\n",
       "  0.08133401721715927,\n",
       "  0.07598061859607697,\n",
       "  0.07033642381429672,\n",
       "  0.0639076679944992],\n",
       " 'sparse_categorical_accuracy': [0.8850600123405457,\n",
       "  0.9476799964904785,\n",
       "  0.9588000178337097,\n",
       "  0.9650599956512451,\n",
       "  0.9695000052452087,\n",
       "  0.9732000231742859,\n",
       "  0.9751399755477905,\n",
       "  0.9767400026321411,\n",
       "  0.9784600138664246,\n",
       "  0.9805600047111511],\n",
       " 'val_loss': [0.17553071677684784,\n",
       "  0.12730850279331207,\n",
       "  0.10716698318719864,\n",
       "  0.10026291757822037,\n",
       "  0.09616535156965256,\n",
       "  0.09210052341222763,\n",
       "  0.08714580535888672,\n",
       "  0.0849992111325264,\n",
       "  0.08159372210502625,\n",
       "  0.08098296821117401],\n",
       " 'val_sparse_categorical_accuracy': [0.9492999911308289,\n",
       "  0.965499997138977,\n",
       "  0.9696000218391418,\n",
       "  0.9710000157356262,\n",
       "  0.9731000065803528,\n",
       "  0.9743000268936157,\n",
       "  0.9742000102996826,\n",
       "  0.9768000245094299,\n",
       "  0.9764000177383423,\n",
       "  0.9794999957084656]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs = 10,\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf94f8f",
   "metadata": {},
   "source": [
    "To make sure the model can fulfill its purpose, we evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20881869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0777 - sparse_categorical_accuracy: 0.9791\n",
      "test loss, test acc: [0.0777440294623375, 0.9790999889373779]\n",
      "Generate predictions for 3 samples\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37472c2",
   "metadata": {},
   "source": [
    "Finally, we try to make a prediction. First, we isolate a sample and plot it, to see it for ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf9eb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sample = x_test[9:10]\n",
    "image = sample[0]\n",
    "plt.imshow(image, cmap='gray_r')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052b3af",
   "metadata": {},
   "source": [
    "Then we have the model make a prediction. The output of the model making a prediction will be the probabilities, that the sample is of a certain class. Here, we have classes 0 to 9, representing the corresponding digits. The output (encapsulated into another array in the output of the predict function) is an array of length 10, where arr[i] is the probability of the sample belonging to class i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87717b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5321417e-12, 1.9668277e-13, 4.0530330e-09, 3.3098651e-08,\n",
       "        1.4859749e-04, 3.7960586e-09, 1.5546140e-12, 2.4480411e-04,\n",
       "        5.9366334e-07, 9.9960595e-01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(sample)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e3e73",
   "metadata": {},
   "source": [
    "Hopefully, the highest probability should represent the class of our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05920b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shows a 9.\n"
     ]
    }
   ],
   "source": [
    "predictedclass = prediction[0].argmax()\n",
    "print(f\"Sample shows a {predictedclass}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb50f40",
   "metadata": {},
   "source": [
    "If all of that worked, we have a functioning model and can go on to the next step: Converting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176b0e5",
   "metadata": {},
   "source": [
    "## 2. Converting the model to ONNX\n",
    "\n",
    "At the moment, our model does not exist outside of this runtime. Usually, before converting, we would have to save the model first. Our first conversion needed is however to convert our model into ONNX format, which can be easily done via the tf2onnx python package. Therefore, we first convert and then save our model.\n",
    "\n",
    "We have to install some new packages first. Note, that tf2onnx might install a downgrade version of some other packages used by tensorflow, which may appear to be a dependency conflict first, but if you just \"reinstall\" the tensorflow package and let it install the newer versions of its dependencies, everything should work out fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e3e56",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install tf2onnx\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de70f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\baumstark\\TinyML\\venv\\Lib\\site-packages\\tf2onnx\\tf_loader.py:68: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Users\\baumstark\\TinyML\\venv\\Lib\\site-packages\\tf2onnx\\tf_loader.py:72: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rewriter <function rewrite_constant_fold at 0x000002661EB4B380>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, opset=13)\n",
    "onnx.save(onnx_model, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b13ff",
   "metadata": {},
   "source": [
    "## 3. Optimization\n",
    "\n",
    "After converting the model to the ONNX format, now would be time to optimize the model to reduce its size and complexity. This could be done for example using the [ONNX Optimizer](https://github.com/onnx/optimizer?tab=readme-ov-file#onnx-optimizer). However, it has not yet been evaluated (within this work), how to best optimize a model for Tiny ML. Therefore, this step will be skipped for now.\n",
    "\n",
    "## 4. Converting the model to C code\n",
    "\n",
    "To convert our (optimized) ONNX model to C code, we use [onnx2c](https://github.com/kraiskil/onnx2c). Therefore, we first have to install onnx2c and then use it, to generate a C file containing our model. Note that this has only be successful on a Linux PC so far.\n",
    "\n",
    "### Get and build onnx2c - Linux\n",
    "\n",
    "To get and build onnx2c on a Linux PC, first install the ProtocolBuffers libraries, a dependecy of onnx2c, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4393ba",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sudo apt install libprotobuv-dev protobuf-compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc3afb",
   "metadata": {},
   "source": [
    "Then, clone the onnx2c github repository and update the submodules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b40833",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/kraiskil/onnx2c.git\n",
    "cd onnx2c\n",
    "git submodule update --init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124dd58",
   "metadata": {},
   "source": [
    "Finally, build onnx2c using CMake like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3248376",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir build\n",
    "cd build\n",
    "cmake -DCMAKE_BUILD_TYPE=Release ..\n",
    "make onnx2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dc21f",
   "metadata": {},
   "source": [
    "### Get and build onnx2c - Windows\n",
    "#### ProtocolBuffers\n",
    "Again, we first have to install the ProtocolBuffers C++ libraries including the protoc compiler, as they are a dependency of onnx2c. We need CMake, Ninja and MSYS2 for the following steps.\n",
    "In an MSYS2 terminal (msys64/mingw64.exe), clone the protobuf git repository to a directory of your preference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df88c8b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/protocolbuffers/protobuf.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a597be",
   "metadata": {},
   "source": [
    "Then, we have to configure the build. Therefore, navigate into the protobuf source directory (probably `cd protobuf`) and enter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e6bbc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cmake -S . -B build -G Ninja -DCMAKE_CXX_STANDARD=17 -DCMAKE_INSTALL_PREFIX=../install -DCMAKE_BUILD_TYPE=Release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf157e",
   "metadata": {},
   "source": [
    "If the previous step was successful, we can now compile the code using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79132cf2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cmake --build build --parallel 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584edfdf",
   "metadata": {},
   "source": [
    "The official instructions include performing tests at this point. There will be no tests recognized when using the following command, and it therefore could probably be omitted. Just to be safe, we won't skip this step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eb04e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ctest --test-dir build --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f93c6",
   "metadata": {},
   "source": [
    "Finally, we can install the libraries using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893689d2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cmake --install build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545fad8",
   "metadata": {},
   "source": [
    "If all of these steps were successful, the libraries and the protoc binary should now be in a directory called \"install\", at the same directory level as the protobuf directory.\n",
    "\n",
    "#### onnx2c\n",
    "Clone the repository and initiate git submodules with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169b816",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/kraiskil/onnx2c.git\n",
    "cd onnx2c\n",
    "git submodule update --init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccce77f",
   "metadata": {},
   "source": [
    "\n",
    "To make onnx2c compatible with windows, we first make some small changes to the code. Note that there are efforts to make onnx2c compatible with windows, so the following might not be necessary anymore at the time you are following these instructions.\n",
    "Open the repository in a code editor like Visual Studio Code and perform the following changes:\n",
    "1. Search for the macro `ERROR`. The name of this macro conflicts with some windows internals. Therefore, we need to change it. Change all occurences (use the code editors functionality to do so), for example to `ONNX2C_ERROR`.\n",
    "2. We need to ensure that strings etc. are encoded as unicode in order to prevent conversion issues. Therefore, we first create a new directory called \"platform\" under \"onnx2c/src/\". Inside this platform, we create a new header file called \"windows_common.h\" with the following content:\n",
    "    ```C\n",
    "    #pragma once\n",
    "    #ifdef _WIN32\n",
    "    #ifndef UNICODE\n",
    "    #define UNICODE\n",
    "    #endif\n",
    "    #ifndef _UNICODE\n",
    "    #define _UNICODE\n",
    "    #endif\n",
    "    #ifndef WIN32_LEAN_AND_MEAN\n",
    "    #define WIN32_LEAN_AND_MEAN\n",
    "    #endif\n",
    "    #include <windows.h>\n",
    "    #endif\n",
    "    ```\n",
    "3. We now have to include the previously created header in the source files where it's necessary (All files that use strings, windows APIs, windows types or macros, or third-party includes that do any of the previous). At the time this instructions are written, those files should be:\n",
    "    - error.h\n",
    "    - graph.cc\n",
    "    - main.cc\n",
    "    - node.cc\n",
    "    - options.cc\n",
    "    - tensor.cc\n",
    "    - (util.cc)\n",
    "    - util.h\n",
    "\n",
    "Now, we should be able to configure the build of onnx2c. In your MSYS2 terminal, move to the root directory of the onnx2c repository and enter the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cf880",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir build\n",
    "cd build\n",
    "cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"your/protobuf/install/path\" -DHAVE_STD_REGEX=ON -DRUN_HAVE_STD_REGEX=1 -G \"MSYS Makefiles\" .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df244e85",
   "metadata": {},
   "source": [
    "Where you exchange `\"your/protoc/install/path\"` for the path of the `install` directory you previously installed your protobuf in.\n",
    "Finally, you should be able to build using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d25aab",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "make onnx2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef0468",
   "metadata": {},
   "source": [
    "### Convert model\n",
    "\n",
    "Now we can use onnx2c to generate a C file containing the model. We can do so from the command line in the directory where the onnx2c executable is built to as follows on Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349c9a1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./onnx2c path/to/model.onnx > model.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17ccf8",
   "metadata": {},
   "source": [
    "For the same command on windows, just use the `.exe` ending for the binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a184155",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./onnx2c.exe path/to/model.onnx > model.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a99e1",
   "metadata": {},
   "source": [
    "## 5. Use model in C code\n",
    "The C file generated by onnx2c contains everything the model needs to run. Note, that the model cannot be changed or trained anymore. The model can, however be used to make a prediction. The generated C file contains a function called `void entry(...)` (at the end of the file). Including the generated C file in your code (via writing a header file or directly), you can call this function with a sample as input argument. The function will (in our case) return an array of probabilities for the corresponding classes, as explained before.\n",
    "\n",
    "To have something to predict, we will use our TensorFlow MNIST dataset and save our previously used sample in a format, that it can be copied and directly used as a C array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17150124",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.c\", 'a') as file:\n",
    "    file.write(\"const float sample[1][28][28] = {{\")\n",
    "\n",
    "    for i in range(27):\n",
    "        string_with_seperator = ','.join(sample[0][i].astype(str))\n",
    "        file.write(\"{\")\n",
    "        file.write(string_with_seperator)\n",
    "        file.write(\"},\\n\")\n",
    "\n",
    "    string_with_seperator = ','.join(sample[0][i].astype(str))\n",
    "    file.write(\"{\")\n",
    "    file.write(string_with_seperator)\n",
    "    file.write(\"}\")\n",
    "\n",
    "    file.write(\"}};\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e6c11",
   "metadata": {},
   "source": [
    "Now we can copy the generated array from the \"sample.c\" file and use it hardcoded for the moment in our C file. This is only for proof of concept and of course not recommended for real projects.\n",
    "The following C file predicts the class of the sample:\n",
    "\n",
    "```C\n",
    "#include <stdio.h>\n",
    "#include \"model.c\"\n",
    "\n",
    "// Copy sample here like so:\n",
    "// const float sample[1][28][28] = ... \n",
    "\n",
    "int main(void) {\n",
    "    float tensor_output[1][10] = {{}};\n",
    "    entry(sample, tensor_output);\n",
    "    for (int i = 0; i < 10; i++)\n",
    "    {\n",
    "        printf(\"%d: %f\\n\", i, tensor_output[0][i]);\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "You can find an example in the `example` directory of this repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
